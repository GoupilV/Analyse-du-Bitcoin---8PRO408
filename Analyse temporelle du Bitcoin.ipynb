{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import des librairies",
   "id": "68ae61be94de853f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "! Peut être nécessaire sur Windows. Il faut redémarrer le kernel Jupyter après installation !",
   "id": "56e41c95e28447a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install --upgrade nbformat",
   "id": "b07d767b48074ad6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extraction du fichier CSV à partir du fichier .zip brut\n",
    "Le fichier CSV ayant une taille de 383,7 Mo, il est impossible de le stocker sur Github. En effet, Github bloque l'upload des fichiers de plus de 100 Mo. Afin de pouvoir reproduire l'analyse des données et le traitement effectué dans ce Notebook, nous allons donc utiliser le fichier au format .zip tel que récupéré sur Kaggle.\n",
    "Si une copie locale des données au format CSV existe, alors cette copie est utilisée. Sinon, le fichier CSV est extrait lors de la première exécution du Notebook."
   ],
   "id": "32b7fc1297fa7c64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not os.path.exists(\"data/raw/btcusd_1-min_data.csv\"):\n",
    "    print(\"Fichier CSV inexistant, extraction à partir du fichier .zip ...\")\n",
    "    with zipfile.ZipFile(\"data/raw/btcusd_1-min_data_11_30_2025.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"data/raw/\")\n",
    "        print(\"Fichier CSV créé !\")\n",
    "else:\n",
    "    print(\"Fichier CSV déjà existant, poursuite de l'exécution ...\")\n"
   ],
   "id": "6aafac071e994813",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analyse préliminaire",
   "id": "f7583b87ac9df86d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Chargement des données CSV dans un DataFrame:",
   "id": "6163f829f5136254"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_raw = pd.read_csv(\"data/raw/btcusd_1-min_data.csv\")",
   "id": "a33dfe743fc609e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_raw.info()",
   "id": "1e3f088ab8079fa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "length_df_raw = len(df_bitcoin_raw)\n",
    "print(length_df_raw)"
   ],
   "id": "721403c76dc98bc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nous obtenons donc un DataFrame comportant 7 317 759 lignes, avec 6 colonnes qui sont: `Timestamp`, `Open`, `High`, `Low`, `Close` et `Volume`\n",
    "Toutes ces colonnes sont pour l'instant de `dtypes: float(64)`."
   ],
   "id": "55fcfc15516c5d87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Statistiques descriptives du DataFrame",
   "id": "bbbfc13e48913e29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_raw.describe()",
   "id": "54730deed67c22e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_raw.dtypes",
   "id": "3343ba845dea6a9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Identification des valeurs manquantes",
   "id": "61235d047772793d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(df_bitcoin_raw.isna().sum())",
   "id": "dc68ce4d40daf30d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On peut observer qu'il n'y a aucune valeurs manquantes.",
   "id": "79dc1da345afcfd7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Identifications des doublons\n",
    "Il n'est pas pertinent d'étudier les doublons des colonnes numériques `Open`, `High`, `Low`, `Close` et `Volume`.\n",
    "En effet, le Bitcoin peut très bien avoir eu le même prix ou volume plusieurs fois depuis que les données ont été collectées.\n",
    "Nous allons cependant étudier la colonne `Timestamp`, représentant un instant T où les données ont été relevées."
   ],
   "id": "86beef5d4cd65fe9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(df_bitcoin_raw[\"Timestamp\"].iloc[0])\n",
   "id": "881bb05e6e84d35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "On remarque que les valeurs contenues dans la colonne `Timestamp` sont au format Timestamp Unix, qui associe un nombre réel au temps mesuré depuis le 1er janvier 1970 à 00:00:00.\n",
    "Ces valeurs sont donc supposées uniques (un instant T correspondant à un nombre réel unique), nous pouvons donc facilement vérifier l'existance de doublons."
   ],
   "id": "2d71cab3f08e3e48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unique_timestamps = df_bitcoin_raw[\"Timestamp\"].nunique()\n",
    "print(f\"Longueur du DataFame et nombre de Timestamp différent équivalent ? {unique_timestamps == length_df_raw}\")\n",
    "print(f\"Nombre de lignes doublons: {df_bitcoin_raw.duplicated().sum()}\")"
   ],
   "id": "50116ba6afc39507",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Le nombre de valeurs uniques de Timestamp étant égal au nombre de lignes dans le DataFrame et n'ayant pas de lignes en double, nous pouvons estimer qu'il n'existe pas de lignes dupliquées dans les données utilisées.",
   "id": "ba6bcfe6f7c5f23b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conversion de \"Timestamp\" (float64, unix epoch time) en DateTime\n",
    "Afin de poursuivre l'analyse sans modifier le DataFrame de base, nous utiliserons désormais une copie."
   ],
   "id": "71e5d98ca45310d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_modified = df_bitcoin_raw.copy()",
   "id": "1695ccd7cba108fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "La conversion du Timestamp en format lisible nous permet une meilleure analyse des données. Il est nécessaire de convertire ce Timestamp de `float` à `int`. Ce timestamp correspond au format Unix équivalent aux secondes, ce qui doit être passé en argument.",
   "id": "d4e6a8aa81392aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_modified[\"Timestamp\"] = pd.to_datetime(df_bitcoin_modified[\"Timestamp\"].astype(int), unit=\"s\")",
   "id": "5b3e9e9e31b3d8c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_modified",
   "id": "5107dd76ba64f2ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_modified.dtypes",
   "id": "b53e64e32155ab8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analyse de l'existence de périodes manquantes\n",
    "Les données issues de Timestamp étant désormais dans un format lisible et reconnaissable par Pandas, nous pouvons donc vérifier si des périodes de temps sont manquantes.\n",
    "\n",
    "Pour cela, nous allons trier les dates dans un ordre ascendant (dans les cas où les lignes ne sont pas correctement \"rangées\"), puis nous pourrons récupérer les valeurs minimum et maximum afin de créer une `Serie` temporelle équivalente.\n",
    "\n",
    "En comparant notre colonne `Timestamp` à cette série, nous pourrons identifier les périodes temporelles manquantes si elles existent."
   ],
   "id": "21fd3b9adc68fd79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "serie_timestamp = df_bitcoin_modified['Timestamp'].sort_values(ascending=True)",
   "id": "708b3e240d6acd02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "first_timestamp = serie_timestamp.iloc[0]\n",
    "last_timestamp = serie_timestamp.iloc[-1]\n",
    "print(f\"Première valeur Timestamp : {first_timestamp}\")\n",
    "print(f\"Dernière valeur Timestamp : {last_timestamp}\")"
   ],
   "id": "db1233353d23bec6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Créons maintenant la série temporelle permettant de vérifier les différences, en utilisant les valeurs trouvées.",
   "id": "631a738b0df1df04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "serie_timestamp_difference = pd.Series(pd.date_range(start=first_timestamp, end=last_timestamp, freq='min'))\n",
    "print(serie_timestamp_difference.head())"
   ],
   "id": "c8853fe3835d1d0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vérifions désormais la différence.",
   "id": "c8d32cda076a1e2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "missing_periods = serie_timestamp_difference[~serie_timestamp_difference.isin(serie_timestamp)]\n",
    "if len(missing_periods) > 0:\n",
    "    print(f\"Il y a {len(missing_periods)} périodes manquantes au total\")\n",
    "\n",
    "    # Vérifions si ces périodes manquantes sont réunies en un seul 'bloc' ou si elles sont dispersées\n",
    "    missing_sorted = missing_periods.sort_values(ascending=True) # Tri ascendant pour s'assurer de la cohérence\n",
    "\n",
    "    # On sait que l'intervalle attendu correspond à 1 minute, puisque notre serie_timestamp_difference a été créé avec une\n",
    "    # fréquence d'1 minute\n",
    "    expected_interval = pd.Timedelta(minutes=1)\n",
    "\n",
    "    # Si la différence est égale à 1 minute, les périodes manquantes sont dans le même 'bloc'\n",
    "    differences = missing_sorted.diff()\n",
    "\n",
    "    # On calcule le nombre de 'blocs' où l'intervalle ne correspond pas à la valeur attendue\n",
    "    num_blocks = (differences != expected_interval).sum()\n",
    "    print(f\"Il y a {num_blocks} blocs de temps manquants\")\n",
    "\n",
    "    # Nous cherchons maintenant à identifier les débuts et fins des blocs de temps manquants\n",
    "    block_starts = missing_sorted.loc[differences != expected_interval].index\n",
    "    for i, start_index in enumerate(block_starts):\n",
    "        # On récupère le timestamp de début du bloc actuel\n",
    "        start_timestamp = missing_sorted.loc[start_index]\n",
    "\n",
    "        # On vérifie s'il y a un bloc suivant, `-1` car l'index débute à `0`\n",
    "        if i < len(block_starts) - 1:\n",
    "            next_start = block_starts[i + 1]\n",
    "            # Le timestamp de fin est celui juste avant le début du prochain bloc\n",
    "            end_index = missing_sorted.index[missing_sorted.index.get_loc(next_start) - 1]\n",
    "            end_timestamp = missing_sorted.loc[end_index]\n",
    "        else:\n",
    "            # Pour le dernier bloc, la fin est la dernière période manquante\n",
    "            end_timestamp = missing_sorted.iloc[-1]\n",
    "            print(f\"Bloc {i+1}: de {start_timestamp} à {end_timestamp}\")\n",
    "else:\n",
    "    print(\"Aucune période manquante détectée\")\n"
   ],
   "id": "f644d65871f797a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nous observons donc un bloc de 1160 minutes consécutives pendant lesquelles les données n'ont pas été relevées.\n",
    "\n",
    "Cela peut s'expliquer par une panne de la plateforme d'échange de cryptomonnaies utilisée pour collecter les données ou un problème technique rencontré par l'outil utilisé pour collecter les données."
   ],
   "id": "a98f31aeab647969"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "missing_pct = len(missing_periods) / unique_timestamps * 100\n",
    "print(f\"Ces périodes manquantes correspondent à {round(missing_pct, 5)}% des données totales\")"
   ],
   "id": "584ba5f0c775a17b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "L'existence de ces périodes manquantes ne posera plus de problèmes plus tard, lorsque nous réaliserons des agrégations temporelles.",
   "id": "1d444800578167bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Utilisation de Timestamp comme index\n",
    "Afin de pouvoir analyser plus finement les données, nous utiliserons les valeurs de la colonne `Timestamp` comme index de notre DataFrame."
   ],
   "id": "b47d54ef12a3bb02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_modified.set_index(\"Timestamp\", inplace=True)",
   "id": "ea12587ad98190c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Interpolation linéaire afin de combler les 1160 minutes manquantes\n",
    "L'interpolation linéaire permet de remplir les valeurs manquantes à l'aide d'une fonction affine estimé à partir des données connues du DataFrame."
   ],
   "id": "fe3ee5382ab763b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_modified= df_bitcoin_modified.resample('1min').interpolate(method='linear')",
   "id": "7bfabb47837d1fe2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nous pouvons vérifier si les valeurs manquantes ont bien été comblées.",
   "id": "6e7c262c261bcce9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Nombre de lignes du DataFrame brut: {length_df_raw}\")\n",
    "print(f\"Nombre de lignes du DataFrame après interpolation linéaire: {len(df_bitcoin_modified)}\")\n",
    "print(f\"Différence entre les deux valeurs: {len(df_bitcoin_modified) - length_df_raw}\")"
   ],
   "id": "cf285e147a1e8138",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nous obtenons bien 1160 nouvelles lignes.",
   "id": "5bc4a60fdaac6290"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse univariée\n",
    "## Analyse de la distribution\n",
    "Nous allons dans un premier temps analyser la forme de la distribution des données.\n",
    "\n",
    "Pour cela nous utiliserons une fonction retournant un DataFrame identifiant les valeurs `skew()` et `kurt()` pour chaque colonne de notre DataFrame."
   ],
   "id": "5092fc39b9e1dc26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def identify_distribution_to_df(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Analyse les colonnes numériques d'un DataFrame et identifie leurs valeurs skew\n",
    "    et kurt. Retourne les valeurs dans un nouveau DataFrame.\\n\n",
    "    skew: Asymétrie (0 = symétrique)\\n\n",
    "    kurt: Aplatissement (>3 = queues épaisses)\n",
    "    :param df: DataFrame\n",
    "    :type df: pandas.DataFrame\n",
    "    :return: DataFrame contenant les noms des colonnes d'entrée, leurs valeurs\n",
    "     skew et kurt.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    distribution_list = []\n",
    "    for series_name, series in df.select_dtypes(include=np.number).items():\n",
    "        skew = series.skew()\n",
    "        kurt = series.kurt()\n",
    "\n",
    "        column_dict = {\"Column\": series_name, \"skew\": skew, \"kurt\": kurt}\n",
    "        distribution_list.append(column_dict)\n",
    "\n",
    "    df_stats = pd.DataFrame(distribution_list)\n",
    "\n",
    "    return df_stats\n"
   ],
   "id": "27cb4b1782cd9ab0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_bitcoin_stats = identify_distribution_to_df(df_bitcoin_modified)\n",
    "df_bitcoin_stats"
   ],
   "id": "1ac2a1bb03849630",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nous pouvons observer une forte asymétrie (valeur `skew`) pour chacune de nos colonnes, ainsi que la présence d'une queue à droite (valeur `skew` > 0) pour chaque colonne étudiée.\n",
    "\n",
    "Nous pouvons observer la forme de la distribution à l'aide de Violin plots. Un échantillonnage aléatoire est utilisé afin de n'étudier que 100000 (valeur `sample_size`) points au lieu de plus de 7.3 millions, accélerant grandement l'affichage des visualisations.\n",
    "\n",
    "Un `random_state` est défini à `42` pour la reproductibilité de l'échantillonnage."
   ],
   "id": "b8c657c525803ae9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_size = 100_000\n",
    "df_sample_violins = df_bitcoin_modified.sample(n=sample_size, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 6))\n",
    "\n",
    "for i, col in enumerate(['Open', 'High', 'Low', 'Close', 'Volume']):\n",
    "    sns.violinplot(y=df_sample_violins[col], ax=axes[i])\n",
    "    axes[i].set_title(f'Violin plot de {col}\\n(échantillon: {sample_size:,} points)')\n",
    "    axes[i].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "69c384a13262fa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Calcul des Z-Scores\n",
    "Afin d'identifier les outliers (valeurs aberrantes), nous allons calculer le Z-score correspondant à chaque valeur de notre dataset."
   ],
   "id": "a277957012b77df0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_z_scores(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule les z-scores pour chaque colonne d'un DataFrame.\n",
    "    Le z-score indique à combien d'écarts-types une valeur se situe de la moyenne.\\n\n",
    "    Formule: z = (x - moyenne) / écart-type\\n\n",
    "    Un z-score de 0 = valeur égale à la moyenne\\n\n",
    "    Un z-score de 2 = valeur à 2 écarts-types au-dessus de la moyenne\n",
    "    Un z-score > 3 = outlier potentiel\n",
    "    :param df: DataFrame contenant les données à utiliser\n",
    "    :type df: pandas.DataFrame\n",
    "    :return: DataFrame contenant les z-scores pour chaque colonne\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    return (df - df.mean()) / df.std()"
   ],
   "id": "e96af3b7c1e67c06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_z_score = calculate_z_scores(df_bitcoin_modified)\n",
    "print(df_z_score.shape)"
   ],
   "id": "308aa3e371633dcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nous pouvons donc maintenant identifier les valeurs aberrantes, quand la valeur aboslue d'un z-score est supeérieur à 3.",
   "id": "b1d5a7e97309e146"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "outliers_mask = (abs(df_z_score) > 3).any(axis=1)\n",
    "outliers_z_score = df_bitcoin_modified.loc[outliers_mask]\n",
    "\n",
    "print(f\"Nombre de outliers détectés: {len(outliers_z_score)}\")"
   ],
   "id": "322c015969645d2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Etudions et visualisons la distribution de ces outliers.",
   "id": "3ebb14e225054fa9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "outliers_z_score_distribution = identify_distribution_to_df(outliers_z_score)\n",
    "print(outliers_z_score_distribution)"
   ],
   "id": "daa24fbafaebbe2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Les colonnes Open, High, Low et Close présentent de légères asymètries à gauche (`skew` négatif) et des queues plus fines que la normale (`kurt` négatif).\n",
    "\n",
    "Mais la colonne Volume présente une très forte asymétrie à droite (`skew` d'environ 6.64) ainsi qu'une queue épaisse, avec beaucoup d'extrêmes (`kurt` d'environ 134.84)"
   ],
   "id": "d7500cb3339a0ac1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i, col in enumerate(outliers_z_score.columns):\n",
    "    skew_val = outliers_z_score_distribution.loc[outliers_z_score_distribution['Column'] == col, 'skew'].values[0]\n",
    "    kurt_val = outliers_z_score_distribution.loc[outliers_z_score_distribution['Column'] == col, 'kurt'].values[0]\n",
    "\n",
    "    sns.violinplot(y=outliers_z_score[col], ax=axes[i])\n",
    "    axes[i].set_title(f'{col}\\nskew={skew_val:.2f}, kurt={kurt_val:.2f}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9fd737fe7ce2615f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ces outliers sont significatifs de période de comportement extrême du prix du Bitcoin (période de crash, bullrun, etc...).\n",
    "\n",
    "Comme ces valeurs correspondent à des évènements bien réels, la meilleure stratégie à adopter est de les conserver."
   ],
   "id": "551654c0d290b72e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Calcul des quartiles et interprétation\n",
    "Calculons désormais les quartiles, les écarts interquartiles et les bornes inférieurs et supérieurs de nos valeurs."
   ],
   "id": "cf34252a011cb045"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def identify_quartiles(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule les quartiles et les bornes inférieures et supérieures pour chaque colonne d'un DataFrame.\\n\n",
    "    Q1: Premier quartile (25e percentile)\\n\n",
    "    Q3: Troisième quartile (75e percentile)\\n\n",
    "    IQR: Ecart interquartile (Q3 - Q1)\\n\n",
    "    Borne inférieure: Q1 - 1.5 × IQR (valeurs en dessous = outliers)\\n\n",
    "    Borne supérieure: Q3 + 1.5 × IQR (valeurs au-dessus = outliers)\n",
    "    :param df: DataFrame contenant les données à analyser\n",
    "    :type df: pandas.DataFrame\n",
    "    :return: DataFrame contenant les quartiles et bornes pour chaque colonne\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    quartiles_list = []\n",
    "\n",
    "    for series_name, series in df.items():\n",
    "        q1 = series.quantile(0.25)\n",
    "        q3 = series.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_fence = q1 - 1.5 * iqr\n",
    "        upper_fence = q3 + 1.5 * iqr\n",
    "\n",
    "        column_dict = {\n",
    "            \"Colonne\": series_name,\n",
    "            \"Q1\": q1,\n",
    "            \"Q3\": q3,\n",
    "            \"IQR\": iqr,\n",
    "            \"Borne inférieure\": lower_fence,\n",
    "            \"Borne supérieure\": upper_fence\n",
    "        }\n",
    "        quartiles_list.append(column_dict)\n",
    "\n",
    "    df_quartiles = pd.DataFrame(quartiles_list)\n",
    "\n",
    "    return df_quartiles"
   ],
   "id": "6f3d79d4f76efa6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_quartiles = identify_quartiles(df_bitcoin_modified)\n",
    "df_quartiles"
   ],
   "id": "a7317cd71acb8fac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Les colonnes Open, High, Low et Close présentent de très larges écarts interquartiles (`IQR`) d'environ 30230, ce qui signifie qu'environ 50% de nos données de prix se situent entre 443 et 30,680$. Cela montre une très forte volatilité du prix du Bitcoin sur la période mesurée.\n",
    "\n",
    "Si on souhaitait utiliser l'IQR afin d'identifier les outliers, alors tout prix supérieur à environ 76,000$ serait considéré comme anormal, ce qui ne respecte pas la tendance globale du prix du Bitcoin:"
   ],
   "id": "129e06abc89ef5b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mean_upper_fence = df_quartiles.loc[df_quartiles['Colonne'].isin(['Open', 'High', 'Low', 'Close']), 'Borne supérieure'].mean()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df_bitcoin_modified.index, df_bitcoin_modified['Close'], label='Prix Close', linewidth=0.5)\n",
    "plt.axhline(y=mean_upper_fence, color='red', linestyle='--', linewidth=1, label=f'Borne sup. moyenne: {mean_upper_fence:.2f}$')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Prix (USD)')\n",
    "plt.title('Prix du Bitcoin avec borne supérieure moyenne des quartiles')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6d64c19f16c846aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nous pouvons donc en déduire que l'utilisation de l'IQR n'est pas adaptée pour identifier les outliers / anomalies de prix du Bitcoin.\n",
    "\n",
    "Quant au volume, une borne supérieure d'environ 7.21 et un IQR d'environ 2.88 semblent être pertinent pour identifier les anomalies de volume. Nous pouvons le vérifier à l'aide d'une visualisation:"
   ],
   "id": "c7a4c142548a5b5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "volume_upper_fence = df_quartiles.loc[df_quartiles['Colonne'] == 'Volume', 'Borne supérieure'].values[0]\n",
    "\n",
    "# Nécessaire pour éviter de devoir tracer 7.3 millions de points\n",
    "df_hourly_volume = df_bitcoin_modified['Volume'].resample('h').mean()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df_hourly_volume.index, df_hourly_volume, label='Volume (horaire)', linewidth=0.5, alpha=0.7)\n",
    "plt.axhline(y=volume_upper_fence, color='red', linestyle='--', linewidth=2, label=f'Borne sup.: {volume_upper_fence:.2f}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volume (échelle log)')\n",
    "plt.yscale('log')\n",
    "plt.title('Volume du Bitcoin, échelle logarithmique')\n",
    "plt.xlim(df_hourly_volume.index.min(), df_hourly_volume.index.max())\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "e462f27589913dee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Sur le graphique ci-dessus, les valeurs au-dessus de la ligne rouge (représentant la borne supérieure du volume) peuvent être considérée comme des anomalies de volume.",
   "id": "afb6e4d02534b7f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering\n",
    "Maintenant que nous avons analysé en détail les distributions et statistiques de nos données, nous pouvons calculer de nouvelles variables intéressantes.\n",
    "\n",
    "Commençons d'abord par standardiser nos colonnes initiales, en les convertissant en minuscules."
   ],
   "id": "aebc786971376d09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_bitcoin_modified.columns = df_bitcoin_modified.columns.str.lower()\n",
    "print(df_bitcoin_modified.columns)"
   ],
   "id": "b8cdfd5559128621",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `returns`, taux d'évolution du prix par rapport au prix précédent",
   "id": "eb9a70ae55a4f60e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_bitcoin_modified['returns'] = df_bitcoin_modified['close'].pct_change()\n",
    "df_bitcoin_modified['returns'].head()"
   ],
   "id": "135b1140759d5019",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "La première valeur `returns` est un `NaN`, car il n'y a pas de valeur précédente. Nous pouvons transformer cette valeur en `0.0`.",
   "id": "7ace38a051ed0902"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_bitcoin_modified['returns'] = df_bitcoin_modified['returns'].fillna(0.0)\n",
    "df_bitcoin_modified['returns'].head()"
   ],
   "id": "92f9908e3d2992ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## `volatility`, volatilité sur la dernière heure\n",
    "La volatilité est calculée en fonction de l'écart-type des valeurs `returns` sur une fênetre roulante de 60 périodes, soit 60 minutes."
   ],
   "id": "231143c5afa7513f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_bitcoin_modified['volatility'] = df_bitcoin_modified['returns'].rolling(60).std()\n",
    "df_bitcoin_modified['volatility'].iloc[54:65]"
   ],
   "id": "1311a98addfd2a2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Les 60 premières valeurs sont `NaN`, car le nombre de période temporelle nécessaire au calcul n'est pas encore atteint. Les données seront agrégées plus tard, ce qui résolvera tous problèmes éventuels liés à ces valeurs.",
   "id": "afeb95e804038eae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `price_range`, intervalle entre le prix `high` et le prix `low`",
   "id": "106570675260a56e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_bitcoin_modified['price_range'] = df_bitcoin_modified['high'] - df_bitcoin_modified['low']\n",
    "\n",
    "# Echantillon aléatoire, avec seed\n",
    "df_bitcoin_modified['price_range'].sample(n=5, random_state=42)"
   ],
   "id": "ebe1ac9abc26b170",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `close_ma_60`, moyenne mobile sur 60 période du prix `close`",
   "id": "3632279de856fe8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_bitcoin_modified['close_ma_60'] = df_bitcoin_modified['close'].rolling(60).mean()\n",
    "df_bitcoin_modified['close_ma_60'].iloc[54:65]"
   ],
   "id": "602a8ccb0ba210a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ici encore, les 60 premières valeurs sont `NaN`, car il n'y a pas assez de période pour les calculs. Une fois agrégées, ces valeurs disparaitront.",
   "id": "a8bc9b4aa173cabd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `volume_ma_60`, moyenne mobile sur 60 période du volume",
   "id": "b637f9193dc17b4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_bitcoin_modified['volume_ma_60'] = df_bitcoin_modified['volume'].rolling(60).mean()\n",
    "df_bitcoin_modified['volume_ma_60'].iloc[54:65]"
   ],
   "id": "f5f6a79605c01f23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Idem pour cette nouvelle colonne.",
   "id": "30bb84c5bc10391"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_modified['volume_ma_60'].sample(n=5, random_state=42)\n",
   "id": "ca1e567128cecefc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## `volume_log`, afin de réduire la valeur `skew()` de `volume`\n",
    "Comme vu précédement, la valeur `skew()` de la colonne volume était très importante:"
   ],
   "id": "ef31f2f5921a310f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_stats.loc[df_bitcoin_stats['Column'] == 'Volume', 'skew']",
   "id": "274ef4db2e301273",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Une échelle logarithmique permettera de grandement réduire cette valeur:",
   "id": "399635d35a6b9e9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_bitcoin_modified['volume_log'] = np.log(df_bitcoin_modified['volume'] + 1)\n",
    "print(f\"Valeur skew() de volume_log: {df_bitcoin_modified['volume_log'].skew()}\")"
   ],
   "id": "cd86cf3803f7bab1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Résumé des nouvelles colonnes",
   "id": "ad271e8fc136d1af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_modified.columns",
   "id": "b90736d99353d701",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Export du DataFrame modifieé vers un nouveau fichier Parquet\n",
    "Afin de gagner du temps pour de potentielles futures analyses ou pour une meilleure reproductibilité, nous pouvons maintenant exporter le DataFrame `df_bitcoin_modified` sous format Parquet, plus performant que CSV pour de grands jeux de données.\n",
    "\n",
    "Il est cependant impossible de mettre en ligne ces données sur Github, le fichier étant trop volumineux."
   ],
   "id": "6f797c7caafa4647"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_bitcoin_modified.to_parquet('data/clean/bitcoin_1min_clean.parquet', engine='fastparquet', compression='zstd')\n",
    "print(f\"DataFrame exporté en Parquet ...\")"
   ],
   "id": "9d9eb421be188132",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Agrégations temporelles\n",
    "Afin de procéder à des agrégations temporelles dans les meilleures conditions, nous pouvons créer une fonction dédiée nous permettant de recalculer les Features de la section précédente et de manipuler les variables existantes en un ensemble cohérent."
   ],
   "id": "94d848be62d232f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def aggregate_ohlcv(df, freq, ma_windows):\n",
    "    \"\"\"\n",
    "    Agrège correctement des données OHLCV.\n",
    "    :param df: DataFrame avec colonnes open, high, low, close, volume\n",
    "    :type df: pandas.DataFrame\n",
    "    :param freq : fréquence ('h', 'D', 'ME', 'W', etc.) d'agrégation voulue\n",
    "    :type freq: str\n",
    "    :param ma_windows: fenêtres pour moyennes mobiles (ex: [7, 30, 90] pour 7j, 30j ou 90j)\n",
    "    :type ma_windows: list\n",
    "    :return: DataFrame agrégé avec features dérivées\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df_agg = df.resample(freq).agg({\n",
    "        # open doit correspondre à la première valeur open de la fréquence d'agrégation voulue\n",
    "        'open': 'first',\n",
    "\n",
    "        # high doit être la plus grande valeur high sur la même période\n",
    "        'high': 'max',\n",
    "\n",
    "        # low est la valeur low la plus petite sur la même période\n",
    "        'low': 'min',\n",
    "\n",
    "        # close correspond ici à la dernière valeur enregistrée pour close\n",
    "        'close': 'last',\n",
    "\n",
    "        # les volumes de la période d'agrégation doivent être additionnés\n",
    "        'volume': 'sum'\n",
    "    })\n",
    "\n",
    "    # Features Engineering\n",
    "    df_agg['returns'] = df_agg['close'].pct_change()\n",
    "    df_agg['price_range'] = df_agg['high'] - df_agg['low']\n",
    "\n",
    "    for window in ma_windows:\n",
    "        df_agg[f'volatility_{window}'] = df_agg['returns'].rolling(window).std()\n",
    "        df_agg[f'close_ma_{window}'] = df_agg['close'].rolling(window).mean()\n",
    "        df_agg[f'volume_ma_{window}'] = df_agg['volume'].rolling(window).mean()\n",
    "\n",
    "\n",
    "    df_agg['volume_log'] = np.log(df_agg['volume'] + 1)\n",
    "\n",
    "    return df_agg"
   ],
   "id": "5241fde8c0e44514",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cette fonction nous permet de définir une liste de périodes temporelles à utiliser en temps que fenêtre roulante, et nous pouvons calculer les mêmes valeurs dérivées que pour notre DataFrame `df_bitcoin_modified` mais sur des périodes agrégées.",
   "id": "5baa280ef47aa0fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Série agrégée 1 heure, avec fenêtre de 24h et d'une semaine\n",
    "df_bitcoin_1_hour = aggregate_ohlcv(df_bitcoin_modified, 'h', ma_windows=[24, 168])\n",
    "print(f\"DataFrame horaire: {len(df_bitcoin_1_hour):,} lignes\")\n",
    "\n",
    "# Série agrégée 1 jour, avec fenêtre d'une semaine, un mois et trois mois\n",
    "df_bitcoin_1_day = aggregate_ohlcv(df_bitcoin_modified, 'D', ma_windows=[7, 30, 90])\n",
    "print(f\"DataFrame journalier: {len(df_bitcoin_1_day):,} lignes\")\n",
    "\n",
    "# Série agrégée 1 mois, avec fenêtre de trois mois et d'un an\n",
    "df_bitcoin_1_month = aggregate_ohlcv(df_bitcoin_modified, 'ME', ma_windows=[3, 12])\n",
    "print(f\"DataFrame mensuel: {len(df_bitcoin_1_month):,} lignes\")"
   ],
   "id": "c1c3a52ec3138912",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Statistiques descriptives pour l'agrégation 1 heure:",
   "id": "5640d3e4fba67cbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_1_hour.describe()",
   "id": "5fea67841897b135",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Statistiques descriptives pour l'agrégation 1 jour:",
   "id": "ca174b8386af9b82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_1_day.describe()",
   "id": "fa828ec95a3c9e80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Statistiques descriptives pour l'agrégation 1 mois:",
   "id": "3e59a67b7eacf207"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bitcoin_1_month.describe()",
   "id": "57517ac827724fe7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Visualisation interactive de la courbe Close\n",
    "En utilisant nos agrégations, nous pouvons désormais créer des visualisations plus claires qu'en utilisant les données collectées chaque minute. En effet, le grand nombre de point à placer dans les visualisations rend les graphiques difficiles à interpréter. Les agrégations permettent de résoudre ce problème."
   ],
   "id": "b48e5dd0691b2c13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualisation interactive de l’évolution du prix dans le temps (courbe Close) avec Plotly Express",
   "id": "c5a628b3cb48dab4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.scatter(df_bitcoin_1_hour, x=df_bitcoin_1_hour.index, y=\"close\", title=\"Prix de clôture du Bitcoin (horaire)\")\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "fig.update_xaxes(title=\"Date\")\n",
    "fig.update_yaxes(title=\"Prix (USD)\",tickformat=\"$,.0f\", separatethousands=True)\n",
    "fig.show()"
   ],
   "id": "919f0acd3d4d07f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Graphique en bougie du Bitcoin (courbe Close) avec Plotly Graph Objects",
   "id": "ecac6db39b46f415"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = go.Figure(data=[\n",
    "    go.Candlestick(\n",
    "        x=df_bitcoin_1_day.index,\n",
    "        open=df_bitcoin_1_day[\"open\"],\n",
    "        high=df_bitcoin_1_day[\"high\"],\n",
    "        low=df_bitcoin_1_day[\"low\"],\n",
    "        close=df_bitcoin_1_day[\"close\"]\n",
    "    )]\n",
    ")\n",
    "\n",
    "y_min = df_bitcoin_1_day[\"low\"].min()\n",
    "y_max = df_bitcoin_1_day[\"high\"].max()\n",
    "fig.update_layout(\n",
    "    yaxis=dict(\n",
    "        range=[y_min, y_max],\n",
    "        tickformat=\"$,.0f\",\n",
    "        separatethousands=True,\n",
    "        title=\"Prix (USD)\"\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Date\",\n",
    "        rangeslider=dict(\n",
    "            visible=True,\n",
    "            thickness=0.05\n",
    "        )\n",
    "    ),\n",
    "    height=800,\n",
    "    title=\"Prix de clôture du Bitcoin (journalier)\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ],
   "id": "a0e5017ccd09b652",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Identification des périodes de Bull Run et de Crash\n",
    "Afin d'identifier les différentes tendances de marché de manière systèmatique et reproductible, nous pouvons définir une classe et des méthodes utilisant des fenêtres mobiles, des maximums et minimums locaux ainsi que des seuils de gain et de perte sur une période définie.\n",
    "\n",
    "Afin d'établir cette méthode, nous nous sommes inspirés des analyses suivantes :\n",
    "- https://blockworks.co/news/bitcoin-bull-market-drawdowns\n",
    "- https://www.kucoin.com/learn/crypto/the-history-of-bitcoin-bull-runs-and-crypto-market-cycles"
   ],
   "id": "b3a991271009d69e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BitcoinTrendDetector:\n",
    "    \"\"\"\n",
    "    Analyse les données de prix du Bitcoin pour identifier les tendances du marché, telles que les crashs\n",
    "    et les bull runs, en fonction de seuils et de durées prédéfinis.\n",
    "\n",
    "    :ivar df: DataFrame contenant le jeu de données de prix du Bitcoin\n",
    "    :type df: pandas.DataFrame\n",
    "    :ivar price_col: Nom de la colonne de prix à analyser\n",
    "    :type price_col: str\n",
    "    :ivar trends: DataFrame résumant les périodes de tendance identifiées, incluant les dates de début et de fin, la durée et le rendement\n",
    "    :type trends: pandas.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, price_col=\"close\"):\n",
    "        \"\"\"\n",
    "        Initialise l'objet\n",
    "\n",
    "        :param df: DataFrame contenant les données de prix du Bitcoin\n",
    "        :type df: pandas.DataFrame\n",
    "        :param price_col: Nom de la colonne de prix à analyser, par defaut \"close\"\n",
    "        :type price_col: str\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.price_col = price_col\n",
    "        self.trends = None\n",
    "\n",
    "    def identify_trends(\n",
    "            self,\n",
    "            min_crash_timeframe: str,\n",
    "            min_bullrun_timeframe: str,\n",
    "            crash_threshold: float,\n",
    "            bullrun_threshold: float,\n",
    "            window: str):\n",
    "        \"\"\"\n",
    "        Identifie les tendances du marché (crash, bullrun, neutral) en fonction de seuils et durées spécifiés.\n",
    "\n",
    "        :param min_crash_timeframe: Durée minimale pour qu'un crash soit considéré valide ('7D' pour 7 jours)\n",
    "        :type min_crash_timeframe: str\n",
    "        :param min_bullrun_timeframe: Durée minimale pour qu'un bull run soit considéré valide ('30D' pour 30 jours)\n",
    "        :type min_bullrun_timeframe: str\n",
    "        :param crash_threshold: Seuil de perte pour identifier un crash (-0.30 pour -30%)\n",
    "        :type crash_threshold: float\n",
    "        :param bullrun_threshold: Seuil de gain depuis le plus bas pour identifier un bull run (1.0 pour +100%)\n",
    "        :type bullrun_threshold: float\n",
    "        :param window: Fenêtre pour calculer les maximums et minimums roulants ('180D' pour 180 jours)\n",
    "        :type window: str\n",
    "        :return: DataFrame contenant les périodes de tendance identifiées avec leurs caractéristiques\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        df = self.df.copy()\n",
    "        price = df[self.price_col]\n",
    "\n",
    "        # Trouve le maximum local sur la periode etudiee\n",
    "        rolling_max = price.rolling(window=window, min_periods=1).max()\n",
    "        # Calcule le drawdown (perte) depuis le maximum local\n",
    "        drawdown = (price - rolling_max) / rolling_max\n",
    "\n",
    "        # Trouve le minimum local sur la periode etudiee\n",
    "        rolling_min = price.rolling(window=window, min_periods=1).min()\n",
    "        # Calcule le gain depuis le minimum local\n",
    "        gain_from_low = (price - rolling_min) / rolling_min\n",
    "\n",
    "        # Calcule le changement de prix sur la periode etudiee\n",
    "        price_change = price.pct_change(periods=pd.Timedelta(window).days)\n",
    "\n",
    "        # Nouvelle serie initialisee en trend 'neutral'\n",
    "        trend = pd.Series('neutral', index=df.index)\n",
    "\n",
    "        # On identifie les periodes de perte superieures a notre seuil => crash + changement de prix negatif\n",
    "        crash_mask = (drawdown <= crash_threshold) & (price_change < 0)\n",
    "        trend[crash_mask] = 'crash'\n",
    "\n",
    "        # On identifie les periodes de gain superieures a notre seuil => bull run + changement de prix positif\n",
    "        bull_mask = (gain_from_low >= bullrun_threshold) & (price_change > 0)\n",
    "        trend[bull_mask] = 'bullrun'\n",
    "\n",
    "        # Applique le filtre de durée minimale pour les crashs et bull runs\n",
    "        trend = self._apply_duration_filter(trend, 'crash', min_crash_timeframe)\n",
    "        trend = self._apply_duration_filter(trend, 'bullrun', min_bullrun_timeframe)\n",
    "\n",
    "        # Ajoute les colonnes de tendance et métriques au DataFrame\n",
    "        df['trend'] = trend\n",
    "        df['drawdown'] = drawdown\n",
    "        df['gain_from_low'] = gain_from_low\n",
    "        df['price_change'] = price_change\n",
    "\n",
    "        self.df = df\n",
    "        self.trends = self._extract_trend_periods()\n",
    "\n",
    "        return self.trends\n",
    "\n",
    "    @staticmethod\n",
    "    def _apply_duration_filter(trend_series, trend_type, min_duration):\n",
    "        \"\"\"\n",
    "        Filtre les périodes de tendance en fonction d'une durée minimale.\n",
    "\n",
    "        :param trend_series: Série contenant les tendances identifiées\n",
    "        :type trend_series: pandas.Series\n",
    "        :param trend_type: Type de tendance à filtrer ('crash' ou 'bullrun')\n",
    "        :type trend_type: str\n",
    "        :param min_duration: Durée minimale requise ('7D')\n",
    "        :type min_duration: str\n",
    "        :return: Série de tendances filtrée\n",
    "        :rtype: pandas.Series\n",
    "        \"\"\"\n",
    "        result = trend_series.copy()\n",
    "\n",
    "        # Permet la comparaison de temps\n",
    "        min_delta = pd.Timedelta(min_duration)\n",
    "\n",
    "        # Identifie les périodes correspondantes au type de tendance spécifié\n",
    "        is_trend = trend_series == trend_type\n",
    "        trend_changes = is_trend.astype(int).diff().fillna(0)\n",
    "\n",
    "        # Trouve les points de début et de fin de chaque période\n",
    "        starts = trend_series.index[trend_changes == 1]\n",
    "        ends = trend_series.index[trend_changes == -1]\n",
    "\n",
    "        # Vérifie la durée de chaque période et filtre celles trop courtes\n",
    "        for start, end in zip(starts, ends):\n",
    "            duration = end - start\n",
    "            if duration < min_delta:\n",
    "                result.loc[start:end] = 'neutral'\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _extract_trend_periods(self):\n",
    "        \"\"\"\n",
    "        Extrait les périodes de tendance du DataFrame.\n",
    "\n",
    "        :return: DataFrame contenant les informations sur chaque période de tendance (type, dates, durée, prix de début/fin, rendement)\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        trend = self.df['trend']\n",
    "\n",
    "        # On compare chaque valeur a la valeur precedente, ce qui retourne un bool, convertit en 0 si False et 1 si True\n",
    "        trend_changes = (trend != trend.shift()).astype(int)\n",
    "        price_col = self.df[self.price_col]\n",
    "\n",
    "        periods = []\n",
    "        current_trend = trend.iloc[0]\n",
    "        start_date = trend.index[0]\n",
    "\n",
    "        # Parcourt toutes les données pour identifier les changements de tendance\n",
    "        for i in range(1, len(trend)):\n",
    "            if trend_changes.iloc[i]:\n",
    "                # Si la tendance précédente n'était pas neutre, on l'enregistre\n",
    "                if current_trend != 'neutral':\n",
    "                    periods.append({\n",
    "                        'trend': current_trend,\n",
    "                        'start': start_date,\n",
    "                        'end': trend.index[i - 1],\n",
    "                        'duration': trend.index[i - 1] - start_date,\n",
    "                        'start_price': price_col.loc[start_date],\n",
    "                        'end_price': price_col.loc[trend.index[i - 1]]\n",
    "                    })\n",
    "                current_trend = trend.iloc[i]\n",
    "                start_date = trend.index[i]\n",
    "\n",
    "        # Traite la dernière période si elle n'est pas neutre\n",
    "        if current_trend != 'neutral':\n",
    "            periods.append({\n",
    "                'trend': current_trend,\n",
    "                'start': start_date,\n",
    "                'end': trend.index[-1],\n",
    "                'duration': trend.index[-1] - start_date,\n",
    "                'start_price': price_col.loc[start_date],\n",
    "                'end_price': price_col.iloc[-1]\n",
    "            })\n",
    "\n",
    "        periods_df = pd.DataFrame(periods)\n",
    "        # Calcule le rendement en pourcentage pour chaque période\n",
    "        if len(periods_df) > 0:\n",
    "            periods_df['return'] = (periods_df['end_price'] / periods_df['start_price'] - 1) * 100\n",
    "\n",
    "        return periods_df\n"
   ],
   "id": "bfcc5c9ffb58c415",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Les seuils `crash_threshold=-0.20` et `bullrun_threshold=0.20` sont définis de façon à respecter les standars de l'industrie :\n",
    "\n",
    "- https://www.investopedia.com/terms/b/bullmarket.asp\n",
    "- https://www.investopedia.com/terms/b/bearmarket.asp"
   ],
   "id": "4a59c2c2547b1995"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "detector = BitcoinTrendDetector(df_bitcoin_1_day)\n",
    "trends = detector.identify_trends(\n",
    "    min_crash_timeframe='30D',\n",
    "    min_bullrun_timeframe='30D',\n",
    "    crash_threshold=-0.20,\n",
    "    bullrun_threshold=0.20,\n",
    "    window='90D'\n",
    ")"
   ],
   "id": "3b36b96376120071",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trends",
   "id": "2565c2857ef9507d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Maintenant que nous avons identifieé nos périodes de Bull run et de Crash, nous pouvons les tracer (avec une échelle de prix logarithmique pour une meilleure clartée):",
   "id": "e98d71c8cc652f86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(detector.df.index, detector.df[detector.price_col], color='black', linewidth=1.5, label='Prix du Bitcoin')\n",
    "\n",
    "# Permet d'éviter d'avoir plusieurs fois le même label\n",
    "bullrun_label_flag = False\n",
    "crash_label_flag = False\n",
    "\n",
    "for _, period in detector.trends.iterrows():\n",
    "    if period['trend'] == 'bullrun':\n",
    "        label = 'Bullrun' if not bullrun_label_flag else ''\n",
    "        ax.axvspan(period['start'], period['end'], alpha=0.3, color='green', label=label)\n",
    "        bullrun_label_flag = True\n",
    "    elif period['trend'] == 'crash':\n",
    "        label = 'Crash' if not crash_label_flag else ''\n",
    "        ax.axvspan(period['start'], period['end'], alpha=0.3, color='red', label=label)\n",
    "        crash_label_flag = True\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Date', fontweight='bold')\n",
    "ax.set_ylabel('Prix (USD)', fontweight='bold')\n",
    "ax.set_title('Périodes de Bull Run et de Crash du Bitcoin', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "32ec924e090f52ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Cette classe semble détecter avec précisions les Bull runs et Crashes passés. Il ne s'agit pas d'une méthode fiable pour déterminer les trends à venir, en raison de l'utilisation de périodes temporelles importantes.\n",
    "\n",
    "On peut notamment remarquer des phases de Bull Run correspondant à la période 2017 - 2018, suivi d'une phase de plusieurs crashs. On observe un crash moyen début 2020, qui correspond à la crise COVID-19.\n",
    "\n",
    "En somme, le cours du Bitcoin est généralement haussier sur le long terme."
   ],
   "id": "75cc878f232501b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Volume d'échange au cours du temps",
   "id": "b04a6c25ba7cbe3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.line(df_bitcoin_1_hour, x=df_bitcoin_1_hour.index, y=\"volume\", title=\"Volume d'échange du Bitcoin (horaire)\")\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "fig.update_xaxes(title=\"Date\")\n",
    "fig.update_yaxes(title=\"Volume\",separatethousands=True)\n",
    "fig.show()"
   ],
   "id": "a13a3e4b3180e6ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On remarque un pic de volume en février 2014. Cela correspond à un évènement marquant de l'histoire du Bitcoin, le piratage de la plateforme d'échange de cryptomonnaies Mt. Gox, qui causa le vol de plus de 744 000 Bitcoins.",
   "id": "44c6bae4839aeb08"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse de la volatilité et variation relative\n",
    "Nous allons analyser la volatilité pour les agrégations temporelles `df_bitcoin_1_day` et `df_bitcoin_1_hour`:"
   ],
   "id": "8629c15e33f081f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "ax1.plot(df_bitcoin_1_hour.index, df_bitcoin_1_hour['volatility_24'],color='cyan', label='Volatilité (24h)')\n",
    "ax1.set_title('Volatilité court terme (données horaires)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(df_bitcoin_1_day.index, df_bitcoin_1_day['volatility_30'], color='red', label='Volatilité (30j)')\n",
    "ax2.set_title('Volatilité long terme (données journalières)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "5f3a613717068d39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Les données journalières semblent présenter beaucoup moins de \"bruit\" que la volatilité des agrégations horaires. Il semblerait donc que l'agrégation journalière soit plus fiable pour apprécier cette tendance.\n",
    "\n",
    "Traçons maintenant la variation relative en pourcentage du Bitcoin, en fonction des mêmes agrégations:"
   ],
   "id": "8e3bacba10e475b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "ax1.plot(df_bitcoin_1_hour.index, df_bitcoin_1_hour['returns'] * 100, color='purple', label='Variation relative horaire (%)')\n",
    "ax1.set_title('Rendement court terme (données horaires)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(df_bitcoin_1_day.index, df_bitcoin_1_day['returns'] * 100, color='orange', label='Variation relative journalière (%)')\n",
    "ax2.set_title('Volatilité long terme (données journalières)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "fa256f8a687c68c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Corrélations entre prix et volume en fonction de l'agrégation et de la fenêtre utilisée\n",
    "Nous pouvons comparer les coefficients de corrélations entre `volume` et les différentes variables dérivées de `close`, comme `returns` ou `volatility` par exemple."
   ],
   "id": "33774767e15e5ed9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_bitcoin_1_hour['return_abs'] = df_bitcoin_1_hour['returns'].abs()\n",
    "df_bitcoin_1_day['returns_abs'] = df_bitcoin_1_day['returns'].abs()\n",
    "\n",
    "df_corr_1_hour = df_bitcoin_1_hour[['close', 'volume', 'returns', 'return_abs', 'volume_log', 'volatility_24','volatility_168']]\n",
    "df_corr_1_day = df_bitcoin_1_day[['close', 'volume', 'returns', 'returns_abs', 'volume_log', 'volatility_7', 'volatility_30']]\n",
    "\n",
    "corr_1_hour = df_corr_1_hour.corr()\n",
    "corr_1_day = df_corr_1_day.corr()"
   ],
   "id": "455d041e271ec8d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.imshow(\n",
    "    corr_1_hour,\n",
    "    text_auto=\".2f\",\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    zmin=-1, zmax=1,\n",
    "    title=\"Heatmap de la corrélation entre variables de l'agrégation 1 heure\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "id": "9692276445e22b40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.imshow(\n",
    "    corr_1_day,\n",
    "    text_auto=\".2f\",\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    zmin=-1, zmax=1,\n",
    "    title=\"Heatmap de la corrélation entre variables de l'agrégation 1 jour\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "id": "5ad53f002fdef2ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "correlation_movement = df_bitcoin_1_day['volume'].corr(df_bitcoin_1_day['returns_abs'])\n",
    "\n",
    "print(f\"Corrélation volume-prix : {df_bitcoin_1_day['volume'].corr(df_bitcoin_1_day['close']):.3f}\")\n",
    "print(f\"Corrélation volume-mouvement : {correlation_movement:.3f}\")"
   ],
   "id": "63f246dc72d8cfe7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On remarque une plus forte corrélation entre le volume d'échange du Bitcoin et les mouvement de prix. Cependant, cette corrélation n'est pas assez significative pour indiquer un réel impact.",
   "id": "958c5f8477d03ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Heatmap du volume moyen d'échange et heure / jour de la semaine\n",
    "A l'aide de l'agrégation horaire, nous pouvons créer une heatmap du volume moyen d'échange du Bitcoin en fonction de l'heure et du jour de la semaine."
   ],
   "id": "9d594565cd77a6b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_bitcoin_heatmap = df_bitcoin_1_hour.copy()\n",
    "\n",
    "# Extrait l'heure et le nom du jour depuis l'index temporel.\n",
    "df_bitcoin_heatmap['hour'] = df_bitcoin_heatmap.index.hour\n",
    "df_bitcoin_heatmap['day_of_week'] = df_bitcoin_heatmap.index.day_name()\n",
    "\n",
    "# Définit l'ordre chronologique des jours.\n",
    "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Création d'un tableau croisé des moyenne du volume par jour vs heure.\n",
    "pivot_volume = df_bitcoin_heatmap.pivot_table(\n",
    "    values='volume',\n",
    "    index='day_of_week',\n",
    "    columns='hour',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Les jours sont remis dans l'ordre\n",
    "pivot_volume = pivot_volume.reindex(days_order)\n",
    "\n",
    "fig = px.imshow(pivot_volume,\n",
    "                labels=dict(x=\"Heure du jour\", y=\"Jour de la semaine\", color=\"Volume moyen d\\'échange\"),\n",
    "                x=pivot_volume.columns,\n",
    "                y=pivot_volume.index,\n",
    "                aspect=\"auto\",\n",
    "                title=\"Volume d\\'échange moyen du Bitcoin par heure et jour de la semaine\")\n",
    "\n",
    "fig.update_layout(height=500, width=1200)\n",
    "fig.show()\n"
   ],
   "id": "50427bd4e5b1154c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "On observe un pic important du volume d'échange moyen les mardis, mercredis, jeudis (pic de la semaine, avec environ 512.1455 Bitcoin échangés en moyenne) et vendredi.\n",
    "\n",
    "On remarque aussi une faible activité les samedis et dimanches, ainsi que les lundis matin.\n",
    "\n",
    "Ces zones d'activité correspondent aux périodes d'ouvertures des marchés financiers, ce qui semble cohérent puisque le Bitcoin est désormais un produit financier spéculatif à part entière."
   ],
   "id": "1848affaa0b447ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Synthèse\n",
    "- Nous avons traité le fichier brut issu de Kaggle, en identifiant notamment une période de 1160 minutes manquantes. Ce fichier présente plus de 7.3 millions de lignes de données, et une fois chargé, occupe plus de 383 Mo de mémoire vive.\n",
    "- Nous avons calculé des statistiques descriptives sur l'ensemble des colonnes de notre DataFrame. Nous avons aussi calculé les Z-scores et les écarts interquartiles afin d'identifier les valeurs aberrantes.\n",
    "- Ces dernières ne nécessitent pas de traitement particulier, puisque le risque de fausser les données en supprimant ou modifiant des évènements bien réels est important.\n",
    "- Une méthodologie pour identifier systèmatiquement les périodes de haute volatilité (bull run de 2016 à 2018 et crash pendant la période COVID par exemple) a été utilisée.\n",
    "- On peut observer sur le volume d'échange du Bitcoin un pic d'activité en février 2014, ce qui correspond au piratage de la plateforme d'échange Mt. Gox et au vol de plus de 740000 Bitcoins.\n",
    "- Nous pouvons affirmer que le Bitcoin est un produit fortement volatil. Les prix peuvent évoluer très rapidement et de manière très forte.\n",
    "- Il n'existe qu'une faible corrélation entre prix et volume, ce qui semble indiquer que d'autres facteurs externes rentrent en jeu.\n",
    "- Une heatmap du volume moyen d'échange au cours de la semaine a été réalisée. Cette dernière montre un pic d'activité de marché au coeur de la semaine, plus précisement en fin de journée. Cela correspond aux périodes d'ouverture des places financières."
   ],
   "id": "58e8227a90f247f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Export de l'agrégation 1h sous format Parquet pour l'application Streamlit",
   "id": "68579b0e9f59cb07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_bitcoin_1_hour.to_parquet('data/clean/bitcoin_1h_engineered.parquet', engine='fastparquet', compression='zstd')\n",
    "print(f\"DataFrame exporté en Parquet ...\")"
   ],
   "id": "94d6a3297611f5b1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
